# -*- coding: utf-8 -*-
"""NeuroTube-Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fPQOtUlYzxnTOqsmP3rIdKw3-IL7o5Nr

# ðŸ§± 1. Imports and Setup
"""

# Standard and system libraries
import os
import re
import time
import random
import tempfile
import json
import concurrent.futures  # for parallel execution

# External packages for multimedia and audio processing
import yt_dlp  # YouTube video downloader
import whisper  # OpenAI's Whisper model for transcription
import speech_recognition as sr  # Speech-to-text
from pydub import AudioSegment  # Audio conversion
from moviepy.editor import VideoFileClip  # Extract audio from video

# LangChain and OpenAI integration
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
from langchain_core.output_parsers import StrOutputParser
from langchain.agents import initialize_agent, AgentType, tool
from langchain_core.tools import Tool

# Other utilities
from gtts import gTTS  # Text-to-speech
from graphviz import Digraph  # Mind map visualization
import gradio as gr  # UI interface

"""# ðŸ” 2. Set Environment Variables and Initialize Whisper"""

# Load Whisper model once
whisper_model = whisper.load_model("base")

# Set API keys for OpenAI and LangChain
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
os.environ["LANGCHAIN_API_KEY"] = "your-langchain-api-key"
os.environ["LANGCHAIN_PROJECT"] = "youtube-rag-bot"
openai.api_key = os.environ["OPENAI_API_KEY"]

"""# ðŸ’¬ 3. Define Prompt Template for QA Chain"""

# This template is used to answer questions based on video transcripts
PROMPT = PromptTemplate(
    input_variables=["question", "context"],
    template="""Use the transcript below to answer the question. Respond in bullet points if possible.\nTranscript:\n{context}\nQuestion:\n{question}"""
)

"""# ðŸ”— 4. YouTube Video Downloading and Validation"""

# Check if a URL is a valid YouTube URL
def is_valid_youtube_url(url):
    pattern = r"(https?://)?(www\.)?(youtube\.com|youtu\.be)/.+"
    return re.match(pattern, url)

# Download YouTube video using yt_dlp and return metadata
def download_youtube_video(url):
    if not is_valid_youtube_url(url):
        return {"error": "Invalid YouTube URL."}

    # Sleep randomly to avoid bot detection
    time.sleep(random.uniform(4, 6))
    temp_dir = tempfile.mkdtemp()
    output_path = os.path.join(temp_dir, "%(title)s.%(ext)s")

    ydl_opts = {
        'format': 'bestvideo+bestaudio/best',
        'outtmpl': output_path,
        'quiet': True,
        'merge_output_format': 'mp4',
        'noplaylist': True,
        'ratelimit': 512 * 1024,
    }

    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=True)
            filename = ydl.prepare_filename(info).replace(".webm", ".mp4")
            return {
                "title": info.get("title"),
                "uploader": info.get("uploader"),
                "duration": round(info.get("duration", 0) / 60, 2),
                "description": info.get("description", "")[:500],
                "filename": filename
            }
    except Exception as e:
        return {"error": str(e)}

"""# ðŸ—£ï¸ 5. Video Transcription using Whisper"""

# Convert video to audio and transcribe it using Whisper
def transcribe_video(file_path):
    time.sleep(random.uniform(2, 4))
    try:
        audio_path = file_path.replace(".mp4", ".wav")
        video_clip = VideoFileClip(file_path)
        if video_clip.audio is None:
            return {"error": "No audio track found in video."}
        video_clip.audio.write_audiofile(audio_path, verbose=False, logger=None)
        result = whisper_model.transcribe(audio_path, word_timestamps=True)
        os.remove(audio_path)
        return result
    except Exception as e:
        return {"error": f"Error during transcription: {str(e)}"}

"""# ðŸ§  6. Text Processing and Vector Database Creation"""

# Split transcript into chunks for semantic search
def split_transcript_chunks(transcript):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    return splitter.split_text(transcript)

# Build FAISS vector DB for semantic search
def build_vector_db(chunks):
    embeddings = OpenAIEmbeddings(api_key=os.environ["OPENAI_API_KEY"])
    return FAISS.from_texts(chunks, embedding=embeddings)

"""# â“ 7. QA Chain and Summarization Setup"""

# Create Retrieval QA chain with GPT-4
def create_qa_chain(vector_db):
    llm = ChatOpenAI(model_name="gpt-4", temperature=0.3, api_key=os.environ["OPENAI_API_KEY"])
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_db.as_retriever(),
        input_key="query",
        chain_type_kwargs={"prompt": PROMPT}
    )

# Summarize transcript using GPT-4
def summarize_with_langsmith(transcript):
    prompt = ChatPromptTemplate.from_template(
        "You are a helpful assistant. Summarize the following YouTube video transcript in bullet points:\n\n{transcript}"
    )
    llm = ChatOpenAI(model_name="gpt-4", temperature=0.3, api_key=os.environ["OPENAI_API_KEY"])
    chain = prompt | llm | StrOutputParser()
    return chain.invoke({"transcript": transcript})

"""# ðŸ”Š 8. Text-to-Speech and Speech-to-Text Functions"""

# Convert summary text into speech using gTTS
def text_to_speech(text):
    tts = gTTS(text)
    audio_file = tempfile.mktemp(suffix=".mp3")
    tts.save(audio_file)
    return audio_file

# Convert user's spoken question into text
def speech_to_text(audio_file_path):
    recognizer = sr.Recognizer()
    audio = AudioSegment.from_file(audio_file_path)
    wav_path = tempfile.mktemp(suffix=".wav")
    audio.export(wav_path, format="wav")
    try:
        with sr.AudioFile(wav_path) as source:
            audio_data = recognizer.record(source)
            return recognizer.recognize_google(audio_data)
    except sr.UnknownValueError:
        return {"error": "Speech recognition could not understand the audio."}
    except sr.RequestError as e:
        return {"error": f"Speech recognition request failed: {str(e)}"}
    except Exception as e:
        return {"error": f"Unexpected error in speech-to-text: {str(e)}"}

"""# ðŸ—ºï¸ 9. Mind Map Generation"""

# Convert summary into structured JSON
def generate_mind_map_structure(summary_text):
    prompt = ChatPromptTemplate.from_template("""
    Convert the following summary into a detailed mind map structure. Return JSON with:
    - 'main_topic': the central theme
    - 'branches': list of dictionaries with 'title', 'subpoints', and optionally 'examples'.

    Summary:
    {summary}
    """)
    llm = ChatOpenAI(model_name="gpt-4", temperature=0.3, api_key=os.environ["OPENAI_API_KEY"])
    chain = prompt | llm | StrOutputParser()
    response = chain.invoke({"summary": summary_text})
    return json.loads(response)

# Render mind map image using Graphviz
def create_mind_map(summary_text):
    try:
        structured_data = generate_mind_map_structure(summary_text)
        dot = Digraph(comment="Mind Map")
        dot.attr(rankdir='LR', fontsize='12', fontname='Helvetica')
        dot.node("0", structured_data["main_topic"], shape='box', style='filled', fillcolor='lightblue')

        for i, branch in enumerate(structured_data["branches"], 1):
            parent_id = f"{i}"
            dot.node(parent_id, branch["title"], shape='ellipse', style='filled', fillcolor='lightyellow')
            dot.edge("0", parent_id)
            for j, subpoint in enumerate(branch.get("subpoints", []), 1):
                child_id = f"{i}.{j}"
                wrapped = "\n".join([subpoint[k:k+50] for k in range(0, len(subpoint), 50)])
                dot.node(child_id, wrapped, shape='note', fontsize='10')
                dot.edge(parent_id, child_id)

        output_path = tempfile.mktemp(suffix=".png")
        dot.render(filename=output_path, format='png', cleanup=True)
        return output_path + ".png"
    except Exception as e:
        return {"error": f"Error generating mind map: {str(e)}"}

"""# ðŸ§  10. LangChain Tools and Agent Setup"""

# LangChain tools for summarization and QA
@tool
def summarize_transcript_tool(transcript: str) -> str:
    return summarize_with_langsmith(transcript)

@tool
def ask_question_tool(context: str, question: str) -> str:
    chunks = split_transcript_chunks(context)
    vector_db = build_vector_db(chunks)
    qa_chain = create_qa_chain(vector_db)
    return qa_chain.run(question)

# Initialize LangChain agent
llm = ChatOpenAI(model="gpt-4", temperature=0.3, api_key=os.environ["OPENAI_API_KEY"])
tools = [
    Tool.from_function(summarize_transcript_tool, name="summarize_transcript_tool", description="Summarize transcript using GPT-4"),
    Tool.from_function(ask_question_tool, name="ask_question_tool", description="Answer question using transcript context")
]
agent = initialize_agent(tools=tools, llm=llm, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)

"""# ðŸ§  10. LangChain Tools and Agent Setup"""

# LangChain tools for summarization and QA
@tool
def summarize_transcript_tool(transcript: str) -> str:
    return summarize_with_langsmith(transcript)

@tool
def ask_question_tool(context: str, question: str) -> str:
    chunks = split_transcript_chunks(context)
    vector_db = build_vector_db(chunks)
    qa_chain = create_qa_chain(vector_db)
    return qa_chain.run(question)

# Initialize LangChain agent
llm = ChatOpenAI(model="gpt-4", temperature=0.3, api_key=os.environ["OPENAI_API_KEY"])
tools = [
    Tool.from_function(summarize_transcript_tool, name="summarize_transcript_tool", description="Summarize transcript using GPT-4"),
    Tool.from_function(ask_question_tool, name="ask_question_tool", description="Answer question using transcript context")
]
agent = initialize_agent(tools=tools, llm=llm, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)

"""# 11. Main Pipeline Function"""

# Orchestrates the entire process based on user input
def handle_video_task(url, question, task, voice_question_audio=None):
    if not url or not is_valid_youtube_url(url):
        return "Invalid URL", "", "", "", "", None, None

    if voice_question_audio and not question:
        question = speech_to_text(voice_question_audio)
        if "error" in question:
            return question["error"], "", "", "", "", None, None

    metadata = download_youtube_video(url)
    if "error" in metadata:
        return metadata["error"], "", "", "", "", None, None

    with concurrent.futures.ThreadPoolExecutor() as executor:
        transcript_result = executor.submit(transcribe_video, metadata["filename"]).result()
        if "error" in transcript_result:
            return transcript_result["error"], "", "", "", "", None, None

        transcript = transcript_result["text"]
        summary = ""
        audio_path = None
        mind_map_path = None

        try:
            if task == "ask_question" and question:
                summary = agent.invoke(f"Use ask_question_tool to answer this question: '{question}' using this context: {transcript}")
            elif task == "langsmith_summary":
                summary = agent.invoke(f"Use summarize_transcript_tool to summarize this transcript: {transcript}")

            if isinstance(summary, dict) and "output" in summary:
                summary = summary["output"]
        except Exception as e:
            return {"error": f"Error while processing task: {str(e)}"}, "", "", "", "", None, None

        if summary:
            audio_path = text_to_speech(summary)
            mind_map_path = create_mind_map(summary)

    return transcript, summary, metadata["title"], metadata["uploader"], f"{metadata['duration']} min", audio_path, mind_map_path

"""# ðŸ§© 12 . Deploymwnt with Gradio UI"""

# === Gradio UI with Custom Styling ===

# Define the Gradio interface with custom CSS for branding and layout
with gr.Blocks(css="""
    .small-audio {
        max-width: 100% !important;
        height: 150px !important;
        min-width: 320px !important;
    }
    .gradio-container {
        background-color: #f5f9fd;
        font-family: 'Helvetica Neue', sans-serif;
    }
    #main-title {
        color: #92b8dd;
        font-size: 28px;
        text-align: center;
        margin-bottom: 10px;
    }
    .section-box {
        border: 1px solid #d1e3f3;
        padding: 16px;
        border-radius: 10px;
        background-color: #ffffff;
    }
    .submit-button {
        background-color: #92b8dd !important;
        color: white !important;
        font-weight: bold;
    }
""") as demo:

    # Page Title
    gr.Markdown("## ðŸŽ¥ YouTube Video AI Assistant", elem_id="main-title")

    # === Tab 1: Main Interface ===
    with gr.Tab("ðŸŽ¬ Main"):
        with gr.Row(equal_height=True):
            # === Left Column: User Input Section ===
            with gr.Column(scale=3, elem_classes="section-box"):
                # YouTube link input
                youtube_input = gr.Textbox(label="ðŸ”— YouTube URL", placeholder="Paste YouTube video link")

                # Question inputs: text or voice
                with gr.Row():
                    question_input = gr.Textbox(label="Text Question", placeholder="Type your question")
                    voice_input = gr.Audio(label="Voice", type="filepath", elem_classes="small-audio")

                # Task selection: summary or QA
                task_choice = gr.Radio(
                    ["ask_question", "langsmith_summary"],
                    label="Choose a Task",
                    value="langsmith_summary"
                )

                # Submit button
                submit_btn = gr.Button("ðŸš€ Run", elem_classes="submit-button")

            # === Right Column: Video Info Output ===
            with gr.Column(scale=2, elem_classes="section-box"):
                gr.Markdown("### ðŸ“º Video Info")
                title_output = gr.Textbox(label="Title", interactive=False)
                uploader_output = gr.Textbox(label="Uploader", interactive=False)
                duration_output = gr.Textbox(label="Duration", interactive=False)

        # === Row: Summary + Audio Output ===
        with gr.Row():
            with gr.Column(elem_classes="section-box"):
                gr.Markdown("### ðŸ§  AI Summary / Answer")
                response_output = gr.Textbox(label="", lines=6)

            with gr.Column(elem_classes="section-box"):
                gr.Markdown("### ðŸ”Š Listen to the Answer")
                audio_output = gr.Audio(label="AI Voice")

    # === Tab 2: Mind Map ===
    with gr.Tab("ðŸ—º Mind Map"):
        gr.Markdown("### Visual Mind Map")
        mindmap_output = gr.Image(label="Concept Map")

    # === Tab 3: Full Transcript ===
    with gr.Tab("ðŸ“„ Transcript"):
        gr.Markdown("### Full Video Transcript")
        transcript_output = gr.Textbox(label="", lines=25)

    # === Connect Button to Function ===
    submit_btn.click(
        handle_video_task,  # Main function to handle video + task
        inputs=[
            youtube_input, question_input, task_choice, voice_input
        ],
        outputs=[
            transcript_output,        # Full transcript
            response_output,          # Summary or QA answer
            title_output,             # Video title
            uploader_output,          # Channel/uploader
            duration_output,          # Duration of video
            audio_output,             # Text-to-speech output
            mindmap_output            # Generated mind map image
        ]
    )

# === Launch the App ===
if __name__ == "__main__":
    demo.launch(share=True)